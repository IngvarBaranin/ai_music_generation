{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Input, GlobalMaxPool1D, Activation\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../dataset_text/miditokens_waitFix.txt'\n",
    "with open(filename) as f:\n",
    "    miditokens = f.readlines()\n",
    "    \n",
    "miditokens = [tokens.strip().split(' ') for tokens in miditokens]\n",
    "\n",
    "#miditokens = []\n",
    "#for i in range(len(midiTokensWithMeta)):\n",
    "#    if 'timesig:4/4' in midiTokensWithMeta[i]:\n",
    "#        midiInfo = midiTokensWithMeta[i]\n",
    "#        \n",
    "#        miditokens.append(midiTokensWithMeta[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() # token -> int\n",
    "tokenizer.fit_on_texts(miditokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n"
     ]
    }
   ],
   "source": [
    "#sample = miditokens[1]\n",
    "#print(sample)\n",
    "#print(tokenizer.texts_to_sequences([sample])[0]) # Example of token input -> int output\n",
    "#print(\"Sõna sagedus kogu andmestikus\", tokenizer.word_counts['wait:0.0'])\n",
    "#print(\"Sõna -> indeks teisendus\", tokenizer.word_index['wait:0.0'])\n",
    "#print(\"Indeks -> sõna teisendus\", tokenizer.index_word[55])\n",
    "print(len(tokenizer.word_index))\n",
    "#print(tokenizer.index_word)\n",
    "#print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all tokens to ints\n",
    "midiTokensAsInt = tokenizer.texts_to_sequences(miditokens)\n",
    "#midiTokensAsInt = np.array(midiTokensAsInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEQ_LEN = 50\n",
    "#\n",
    "#with open(\"../dataset_text/seq2note_int.txt\", \"a\") as f:\n",
    "#    for song in midiTokensAsInt:\n",
    "#        for i in range(0, len(song) - SEQ_LEN, 1):\n",
    "#            song = [str(token) for token in song]\n",
    "#            f.write(' '.join(song[i:i + SEQ_LEN]) + \", \" + (song[i + SEQ_LEN]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/train-keras-model-with-large-dataset-batch-training-6b3099fdf366\n",
    "\n",
    "# 1721612\n",
    "LINES = 1721612\n",
    "#for song in midiTokensAsInt:\n",
    "#    for idx in range(0, len(song) - SEQ_LEN, 1):\n",
    "#        LINES += 1\n",
    "#print(LINES)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "SEQ_LEN = 50\n",
    "VOCAB_SIZE = len(tokenizer.word_index)\n",
    "steps = LINES // BATCH_SIZE\n",
    "\n",
    "with open(\"../dataset_text/seq2note_int.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "def batchGenerator(trainData, lines, steps, VOCAB_SIZE, LINES, SEQ_LEN=50, BATCH_SIZE=32):\n",
    "    lastLine = 0\n",
    "    while True:\n",
    "        \n",
    "        # https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5\n",
    "        \n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        \n",
    "        for idx in range(lastLine, min(lastLine + BATCH_SIZE, LINES), 1):\n",
    "            sample = lines[idx].split(\", \")\n",
    "            X_train.append([int(i) for i in sample[0].split(\" \")])\n",
    "            y_train.append(int(sample[1]))\n",
    "\n",
    "        X_train = to_categorical(X_train, num_classes=VOCAB_SIZE+1)\n",
    "        y_train = to_categorical(y_train, num_classes=VOCAB_SIZE+1)\n",
    "        \n",
    "        yield (X_train, y_train)\n",
    "        \n",
    "        lastLine += BATCH_SIZE\n",
    "        if lastLine > LINES:\n",
    "            lastLine = 0\n",
    "\n",
    "batchGen = batchGenerator(midiTokensAsInt, lines, steps, VOCAB_SIZE, LINES, SEQ_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = next(batchGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 102/6725 [..............................] - ETA: 9:19 - loss: 4.6499"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    256,\n",
    "    input_shape=(SEQ_LEN, VOCAB_SIZE+1),\n",
    "    return_sequences=True\n",
    "))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(VOCAB_SIZE+1))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "filepath = \"../weights/weights-generator/weights-{epoch:02d}-{loss:.4f}.hdf5\"    \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss', \n",
    "    verbose=0,        \n",
    "    save_best_only=True,        \n",
    "    mode='min'\n",
    ")    \n",
    "callbacks_list = [checkpoint]     \n",
    "#model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list)\n",
    "\n",
    "model.fit(batchGen, \n",
    "          workers=0,\n",
    "          steps_per_epoch = steps,\n",
    "          max_queue_size = 0,\n",
    "          epochs=200,\n",
    "          callbacks=callbacks_list,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"LSTM256-generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"LSTM256-generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(LSTM(\n",
    "#    256,\n",
    "#    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "#    return_sequences=True\n",
    "#))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(256, return_sequences=True))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(256))\n",
    "#model.add(Dense(256))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(VOCAB_SIZE))\n",
    "#model.add(Activation('softmax'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "#\n",
    "## Load the weights to each node\n",
    "#model.load_weights('../weights/weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv57463'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv57463');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQACBABNVHJrAAAAFAD/UQMHCuIA/1gEBAIYCIgA/y8ATVRyawAAAI8A/wMAAOAAQIgAkExQAJA0QIQAgDQAAJBAQIQAgEwAAIBAAACQR0AAkDRAhACARwAAgDQAAJBIQACQQECEAIBIAACAQAAAkEpQAJA0QIQAgDQAAJBAQIQAgEoAAIBAAACQSEAAkDRAhACASAAAgDQAAJBHQACQQECEAIBHAACAQAAAkC1AhACALQCIAP8vAA==');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intToNote = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "songTokens = tokenizer.texts_to_sequences([midi2text(open_midi(\"../testmidis/tetris.mid\"))])[0]\n",
    "pattern = [songTokens[:50]]\n",
    "\n",
    "prediction_output = [intToNote[i] for i in list(pattern[0])]\n",
    "\n",
    "text2midi(prediction_output).show(\"midi\")\n",
    "\n",
    "# generate 500 notes\n",
    "for note_index in range(500):\n",
    "    #prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    #prediction_input = prediction_input / float(VOCAB_SIZE)\n",
    "    prediction_input = to_categorical(pattern, num_classes=VOCAB_SIZE)\n",
    "    \n",
    "    prediction = model.predict(prediction_input, verbose=0)\n",
    "    \n",
    "    index = np.argmax(prediction)\n",
    "    result = intToNote[index]\n",
    "    prediction_output.append(result)\n",
    "    \n",
    "    #print(index)\n",
    "    pattern = np.append(pattern, index)\n",
    "    #print(pattern)\n",
    "    #print(pattern[1:len(pattern)])\n",
    "    pattern = [pattern[1:len(pattern)]]\n",
    "    \n",
    "if 'end' in prediction_output:\n",
    "    print('warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv59085'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv59085');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQACBABNVHJrAAAAFAD/UQMHCuIA/1gEBAIYCIgA/y8ATVRyawAABaUA/wMAAOAAQIgAkExQAJA0QIQAgDQAAJBAQIQAgEwAAIBAAACQR0AAkDRAhACARwAAgDQAAJBIQACQQECEAIBIAACAQAAAkEpQAJA0QIQAgDQAAJBAQIQAgEoAAIBAAACQSEAAkDRAhACASAAAgDQAAJBHQACQQECEAIBHAACAQAAAkEVQAJAtQIQAgC0AAJA5QIIAkE1AhACATQAAkEJAggCAQgCCVYA5AIJWkEVwglWQT2CCVYBPAACQRXCCVpAtcIJVgEUAAIBFAACARQAAgC0AAJA0cIJVkDNwglWAMwCFK5A9cIJVgD0AAZBCcIJVkElwglWQPWCCVYA9AAGQNmCCVYA2AACQNGCCVZAsYIJVgCwAAZAmcIJVgCYAAJAtcIQAgC0AAJA0cIQAgDQAAIA0AACANAAAkDZwhACANgAAkDlwhACAOQAAkD1whACAPQAAkEFwhACQPWCEAIA9AACQOX+EAIA5AACQNmCEAIA2AACQNHCEAIA0AACQLXCEAIBBAACALQAAkCZwhACAJgAAkC1whACALQAAkDRwhACANAAAkDZ/hACANgAAkDlwhACAOQAAkD1whACAPQAAkEJwhACAQgAAgEIAAJA9YIQAgD0AAJA5f4QAgDkAAJA2cIQAgDYAAJA0cIQAgDQAAJAtcIQAgC0AAJAmcIQAgCYAAJAtcIQAgC0AAJA0cIQAgDQAAJA2cIQAgDYAAJA5cIQAgDkAAJA9cIQAgD0AAJBEcIQAgEQAAJA9YIQAgD0AAJA5cIQAgDkAAJA2cIQAgDYAAJA0cIQAgDQAAJAtcIQAgC0AAJAlcIQAgCUAAJAscIQAgCwAAJAzcIIAgDMAAJA0cIIAgDQAAJA2cIIAgDYAAJA4cIIAgDgAAJBAcIJVkDhwglWAOAABkDZwglWANgAAkDRwglWANAAAkDNgglWAMwABkCxwglWAQAAAgCwAAJA/cACQJWCCVYAlAACQLHCCVYAsAAGQM3CCVYAzAACQNHCCVYA0AACQNnCCVYA2AAGQOHCCVYA/AACAOAAAkDtgglWQOHCCVYA4AAGQNnCCVYA2AACQNGCCVYA0AACQM2CCVYAzAAGQLHCCVYA7AACALAAAkD1wAJAlcIJVgCUAAJAscIJVgCwAAZAzcIJVgDMAAJA0cIJVgDQAAJA2cIJVgDYAAZA4f4JVgD0AAIA4AACQP3CCVZA4cIJVgDgAAZA2YIJVgDYAAJA0f4JVgDQAAJAzcIJVgDMAAZAsYIJVgD8AAIAsAACQQHAAkCV/glWAJQAAkCxwglWALAAAkDNwglWAMwABkDRgglWANAAAkDZ/glWANgBWkDhwggCAQAAAkEFwVYA4AIIAkDhwglWAOAAAkDZwglWANgABkDR/glWANAAAkDNgglWAMwAAkCxwglWALAABgEEAAJAmcIQAgCYAAJAtcIQAgC0AAJA0cIQAgDQAAJA2cIQAgDYAAJA5YIQAgDkAAJA9cIQAgD0AAJA/cIQAkD1whACAPQAAkDlwhACAOQAAkDZwhACANgAAkDRwhACANAAAkC1/hACAPwAAgC0AAJAmcACQQHCEAIAmAACQLXCEAIAtAACQNHCEAIA0AACQNmCEAIA2AACQOXCEAIA5AACQPXCEAIBAAACAPQAAkEJgglWQPXCCVYA9AACQOXCCVYA5AAGQNnCCVYA2AACQNHCCVYA0AACQLXCCVYAtAAGAQgAAkERwAJAmcIJVgCYAAJAtcIJVgC0AAJA0cIJVgDQAAZA2cIJVgDYAAJA5cIJVgDkAAJA9cIJVgD0AAYBEAACQSWCCVZA9cIJVgD0AAJA5cIJVgDkAAZA2cIJVgDYAAJA0f4JVgDQAAJAtcIJVgC0AAYBJAACASQAAkCZwglWAJgAAkC1wglWALQAAkDRwglWANACIAP8vAA==');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(prediction_output)\n",
    "genmidistream = text2midi(prediction_output)\n",
    "genmidistream.show(\"midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = genmidistream.write('midi', fp='wow2.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "\n",
    "def open_midi(midi_path):\n",
    "    mf = converter.parse(midi_path)\n",
    "    return mf\n",
    "\n",
    "# Restricts possible velocities to 8 values, keeping the number of unique note events smaller\n",
    "# Resembles ppp, pp, p, mp, mf, f, ff, fff dynamics \n",
    "def vModifier(velocity):\n",
    "    if (velocity == 0):\n",
    "        return 0\n",
    "    \n",
    "    velocity = min(127, ((velocity // 16) + 1) * 16)\n",
    "    return velocity\n",
    "\n",
    "def tModifier(tempo):\n",
    "    if (tempo == 0):\n",
    "        return 0\n",
    "    \n",
    "    tempo = ((tempo // 10) + 1) * 10\n",
    "    return tempo\n",
    "\n",
    "# Check if there are notes which should have ended before given offset\n",
    "def checkForNoteOffEvent(currentOffset, noteOffEvents):\n",
    "    notesToEnd = []\n",
    "    \n",
    "    for noteOffEvent in noteOffEvents: # for (notename, endingOffset)\n",
    "        if noteOffEvent[1] <= currentOffset:\n",
    "            notesToEnd.append(noteOffEvent)\n",
    "            \n",
    "    return notesToEnd\n",
    "\n",
    "def midi2text(midifile):\n",
    "    previousElementOffset = 0.0\n",
    "    offsetChanged = False\n",
    "\n",
    "    tempoRetrieved = False\n",
    "    timeSigRetrieved = False\n",
    "    \n",
    "    currentVelocity = 0\n",
    "\n",
    "    tokens = []\n",
    "    noteOffEvents = []\n",
    "\n",
    "    tokens.append(\"START\")\n",
    "\n",
    "    for element in midifile.flat.elements:\n",
    "        #print(type(element))\n",
    "\n",
    "        currentElementOffset = element.offset\n",
    "\n",
    "        notesToEnd = checkForNoteOffEvent(currentElementOffset, noteOffEvents)\n",
    "\n",
    "        if (len(notesToEnd) != 0):\n",
    "            for noteToEnd in notesToEnd:\n",
    "                difference = float(noteToEnd[1]) - float(previousElementOffset)\n",
    "                if (difference > 0.01):\n",
    "                    tokens.append(\"wait:\" + str(round(difference, 5)))\n",
    "                    previousElementOffset = noteToEnd[1]\n",
    "                tokens.append(\"note:\" + str(noteToEnd[0]) + \":OFF\")\n",
    "                noteOffEvents.remove(noteToEnd)\n",
    "\n",
    "        # If offset has increased and we're looking at new notes, add a wait event before adding the new notes\n",
    "        if (float(currentElementOffset) > float(previousElementOffset + 0.01) and (isinstance(element, note.Note) or isinstance(element, chord.Chord))):\n",
    "            offsetChanged = True\n",
    "            difference = float(currentElementOffset - previousElementOffset)\n",
    "            tokens.append(\"wait:\" + str(round(difference, 5)))\n",
    "\n",
    "        if (isinstance(element, tempo.MetronomeMark) and not tempoRetrieved):\n",
    "            tempoRetrieved = True\n",
    "            tokens.append(\"tempo:\" + str(tModifier(element.number)))\n",
    "\n",
    "        if (isinstance(element, meter.TimeSignature) and not timeSigRetrieved):\n",
    "            timeSigRetrieved = True\n",
    "            tokens.append(\"timesig:\" + str(element.ratioString))\n",
    "\n",
    "        if (isinstance(element, note.Note)): # This is a note event, add a token for this note\n",
    "            if (currentVelocity != vModifier(element.volume.velocity)):\n",
    "                currentVelocity = vModifier(element.volume.velocity)\n",
    "                tokens.append(\"velocity:\" + str(currentVelocity))\n",
    "            tokens.append(\"note:\" + str(element.pitch))\n",
    "            noteOffEvents.append((str(element.pitch), float(currentElementOffset + element.duration.quarterLength), 5))\n",
    "\n",
    "        if (isinstance(element, chord.Chord)): # This is a chord event, add a token for each note in chord\n",
    "            for chordnote in element:\n",
    "                if (currentVelocity != vModifier(element.volume.velocity)):\n",
    "                    currentVelocity = vModifier(element.volume.velocity)\n",
    "                    tokens.append(\"velocity:\" + str(currentVelocity))\n",
    "                tokens.append(\"note:\" + str(chordnote.pitch))\n",
    "                noteOffEvents.append((str(chordnote.pitch), float(currentElementOffset + element.duration.quarterLength)))\n",
    "\n",
    "        if (offsetChanged):\n",
    "            previousElementOffset = currentElementOffset\n",
    "            offsetChanged = False\n",
    "\n",
    "    # Finally make sure that all notes that end after the offset of the last element of mf.flat.elements are given an off event.\n",
    "    for noteToEnd in noteOffEvents.copy():\n",
    "        difference = float(noteToEnd[1]) - float(previousElementOffset)\n",
    "        if (difference > 0.01):\n",
    "            tokens.append(\"wait:\" + str(round(difference, 5)))\n",
    "            previousElementOffset = noteToEnd[1]\n",
    "        tokens.append(\"note:\" + str(noteToEnd[0]) + \":OFF\")\n",
    "        noteOffEvents.remove(noteToEnd)\n",
    "        \n",
    "    if (len(noteOffEvents) != 0):\n",
    "        print(\"Not all notes have note-off events\")\n",
    "\n",
    "    tokens.append(\"END\")\n",
    "    return tokens\n",
    "\n",
    "def text2midi(tokens):\n",
    "    s = stream.Stream()\n",
    "    \n",
    "    currentVelocity = 80\n",
    "\n",
    "    currentOffset = 0\n",
    "    currentToken = 0\n",
    "\n",
    "    for token in tokens:\n",
    "\n",
    "        splitToken = token.split(\":\")\n",
    "\n",
    "        if token.startswith(\"tempo\"):\n",
    "            s.append(tempo.MetronomeMark(number=float(splitToken[1])))\n",
    "\n",
    "        if token.startswith(\"timesig\"):\n",
    "            s.append(meter.TimeSignature(splitToken[1]))\n",
    "            \n",
    "        if token.startswith(\"velocity\"):\n",
    "            currentVelocity = int(splitToken[1])\n",
    "\n",
    "        if token.startswith(\"note\") and not token.lower().endswith(\"off\"):\n",
    "            noteDuration = 0\n",
    "            noteName = splitToken[1]\n",
    "\n",
    "            for element in tokens[currentToken+1:]:\n",
    "                splitToken2 = element.split(\":\")\n",
    "                if (element.startswith(\"wait\")):\n",
    "                    noteDuration += float(splitToken2[1])\n",
    "                if (element.startswith(\"note\") and element.lower().endswith(\"off\")):\n",
    "                    if (noteName == splitToken2[1]):\n",
    "                        newNote = note.Note(nameWithOctave=splitToken[1],  \n",
    "                               quarterLength=float(noteDuration))\n",
    "                        newNote.volume.velocity = currentVelocity\n",
    "                        s.insert(currentOffset, newNote)\n",
    "                        break\n",
    "\n",
    "        if token.startswith(\"wait\"):\n",
    "            currentOffset += float(splitToken[1]) \n",
    "\n",
    "        currentToken += 1\n",
    "\n",
    "    return s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
