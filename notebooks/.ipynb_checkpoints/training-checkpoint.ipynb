{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Input, GlobalMaxPool1D, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../dataset_text/miditokens.txt'\n",
    "with open(filename) as f:\n",
    "    miditokens = f.readlines()\n",
    "    \n",
    "miditokens = [tokens.strip().split(' ') for tokens in miditokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() # token -> int\n",
    "tokenizer.fit_on_texts(miditokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    }
   ],
   "source": [
    "sample = miditokens[1]\n",
    "#print(sample)\n",
    "#print(tokenizer.texts_to_sequences([sample])[0]) # Example of token input -> int output\n",
    "#print(\"Sõna sagedus kogu andmestikus\", tokenizer.word_counts['end'])\n",
    "#print(\"Sõna -> indeks teisendus\", tokenizer.word_index['note:c4:v112'])\n",
    "#print(\"Indeks -> sõna teisendus\", tokenizer.index_word[55])\n",
    "print(len(tokenizer.word_index))\n",
    "#print(tokenizer.index_word)\n",
    "#print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all tokens to ints\n",
    "midiTokensAsInt = tokenizer.texts_to_sequences(miditokens)\n",
    "#midiTokensAsInt = np.array(midiTokensAsInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://medium.com/analytics-vidhya/train-keras-model-with-large-dataset-batch-training-6b3099fdf366\n",
    "#\n",
    "#def batchGenerator(trainData, VOCAB_SIZE, SEQ_LEN=30):\n",
    "#    i = 0\n",
    "#    while True:\n",
    "#        yield loadDataBatch(trainData, VOCAB_SIZE, SEQ_LEN, i)\n",
    "#        i += 1\n",
    "#    \n",
    "#            \n",
    "#def loadDataBatch(trainData, VOCAB_SIZE, SEQ_LEN, i):\n",
    "#    X = []\n",
    "#    y = []\n",
    "#    \n",
    "#    # https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5\n",
    "#    \n",
    "#    song = trainData[i]\n",
    "#    for i in range(0, len(song) - SEQ_LEN, 1):\n",
    "#        X.append(song[i:i + SEQ_LEN])\n",
    "#        y.append(song[i + SEQ_LEN])\n",
    "#            \n",
    "#    input_len = len(X)\n",
    "#    \n",
    "#    # reshape the input into a format compatible with LSTM layers\n",
    "#    X = np.reshape(X, (input_len, SEQ_LEN, 1))\n",
    "#    \n",
    "#    # normalize input\n",
    "#    X = X / float(VOCAB_SIZE)\n",
    "#    y = to_categorical(y, num_classes = VOCAB_SIZE)\n",
    "#    \n",
    "#    return (X, y)\n",
    "#\n",
    "#VOCAB_SIZE = len(tokenizer.word_index)\n",
    "#SEQ_LEN = 20\n",
    "#\n",
    "#batchGen = batchGenerator(midiTokensAsInt, VOCAB_SIZE, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(LSTM(\n",
    "#    256,\n",
    "#    input_shape=(20, 1),\n",
    "#    return_sequences=True\n",
    "#))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(256, return_sequences=True))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(256))\n",
    "#model.add(Dense(256))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(VOCAB_SIZE))\n",
    "#model.add(Activation('softmax'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "#\n",
    "#filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"    \n",
    "#checkpoint = ModelCheckpoint(\n",
    "#    filepath, monitor='loss', \n",
    "#    verbose=0,        \n",
    "#    save_best_only=True,        \n",
    "#    mode='min'\n",
    "#)    \n",
    "#callbacks_list = [checkpoint]     \n",
    "##model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list)\n",
    "#\n",
    "#model.fit(batchGen, \n",
    "#          epochs=200,\n",
    "#          callbacks=callbacks_list,\n",
    "#          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-generator variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index)\n",
    "SEQ_LEN = 100\n",
    "\n",
    "# https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5\n",
    "\n",
    "for song in midiTokensAsInt:\n",
    "    for i in range(0, len(song) - SEQ_LEN, 1):\n",
    "        X.append(song[i:i + SEQ_LEN])\n",
    "        y.append(song[i + SEQ_LEN])\n",
    "\n",
    "input_len = len(X)\n",
    "\n",
    "# reshape the input into a format compatible with LSTM layers\n",
    "X = np.reshape(X, (input_len, SEQ_LEN, 1))\n",
    "\n",
    "# normalize input\n",
    "X = X / float(VOCAB_SIZE)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    256,\n",
    "    input_shape=(X.shape[1], X.shape[2]),\n",
    "    return_sequences=True\n",
    "))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(VOCAB_SIZE+1))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "filepath = \"../weights/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"    \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss', \n",
    "    verbose=0,        \n",
    "    save_best_only=True,        \n",
    "    mode='min'\n",
    ")    \n",
    "callbacks_list = [checkpoint]     \n",
    "model.fit(X, y, epochs=200, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    256,\n",
    "    input_shape=(X.shape[1], X.shape[2]),\n",
    "    return_sequences=True\n",
    "))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(VOCAB_SIZE+1))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# Load the weights to each node\n",
    "model.load_weights('../weights/weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.random.randint(0, len(X)-1)\n",
    "intToNote = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "pattern = midiTokensAsInt[57][:100]\n",
    "#pattern = [i for [i] in pattern]\n",
    "prediction_output = []\n",
    "for tokenInt in pattern:\n",
    "    prediction_output.append(intToNote[tokenInt])\n",
    "\n",
    "# generate 500 notes\n",
    "for note_index in range(500):\n",
    "    prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    prediction_input = prediction_input / float(VOCAB_SIZE)\n",
    "    \n",
    "    prediction = model.predict(prediction_input, verbose=0)\n",
    "    \n",
    "    index = np.argmax(prediction)\n",
    "    result = intToNote[index]\n",
    "    prediction_output.append(result)\n",
    "    \n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv108107'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv108107');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQACBABNVHJrAAAAFAD/UQME0Y0A/1gEBAIYCIgA/y8ATVRyawAAASAA/wMAAOAAQIgAkENwAJBPcIYAgE8AggCQR3CGAIBHAIIAkExwhgCAQwAAgEwAggCQQ3AAkE9whgCAQwAAgE8AggCQQnAAkE5whgCAQgAAgE4AggCQQHAAkExwhgCAQAAAgEwAggCQPnAAkEpwhgCAPgAAgEoAggCQQnAAkE5whgCAQgAAgE4AggCQQHAAkExwngCATACgAIBAAIIAkEhwAJA5cIYAgDkAggCQPHCGAIA8AIIAkEBwhgCASAAAgEAAggCQSHAAkDxwhgCASAAAgDwAggCQR3AAkDdwhgCARwAAgDcAggCQRXAAkDtwhgCARQAAgDsAggCQQ3AAkD5whgCAQwAAgD4AggCQR3AAkDtwhgCARwAAgDsAiAD/LwA=');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text2midi(miditokens[57][:100]).show(\"midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv109999'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv109999');\n",
       "                               mp.base64Load('data:audio/midi;base64,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');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(prediction_output)\n",
    "genmidistream = text2midi(prediction_output)\n",
    "genmidistream.show(\"midi\")\n",
    "#fp = genmidistream.write('midi', fp='uhhh.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "\n",
    "def text2midi(tokens):\n",
    "    s = stream.Stream()\n",
    "    \n",
    "    currentVelocity = 0\n",
    "\n",
    "    currentOffset = 0\n",
    "    currentToken = 0\n",
    "\n",
    "    for token in tokens:\n",
    "\n",
    "        splitToken = token.split(\":\")\n",
    "\n",
    "        if token.startswith(\"tempo\"):\n",
    "            s.append(tempo.MetronomeMark(number=float(splitToken[1])))\n",
    "\n",
    "        if token.startswith(\"timesig\"):\n",
    "            s.append(meter.TimeSignature(splitToken[1]))\n",
    "            \n",
    "        if token.startswith(\"velocity\"):\n",
    "            currentVelocity = int(splitToken[1])\n",
    "\n",
    "        if token.startswith(\"note\") and not token.lower().endswith(\"off\"):\n",
    "            noteDuration = 0\n",
    "            noteName = splitToken[1]\n",
    "\n",
    "            for element in tokens[currentToken+1:]:\n",
    "                splitToken2 = element.split(\":\")\n",
    "                if (element.startswith(\"wait\")):\n",
    "                    noteDuration += float(splitToken2[1])\n",
    "                if (element.startswith(\"note\") and element.lower().endswith(\"off\")):\n",
    "                    if (noteName == splitToken2[1]):\n",
    "                        newNote = note.Note(nameWithOctave=splitToken[1],  \n",
    "                               quarterLength=float(noteDuration))\n",
    "                        newNote.volume.velocity = currentVelocity\n",
    "                        s.insert(currentOffset, newNote)\n",
    "                        break\n",
    "\n",
    "        if token.startswith(\"wait\"):\n",
    "            currentOffset += float(splitToken[1]) \n",
    "\n",
    "        currentToken += 1\n",
    "\n",
    "    return s\n",
    "\n",
    "#text2midi(midiTokens).show(\"midi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
